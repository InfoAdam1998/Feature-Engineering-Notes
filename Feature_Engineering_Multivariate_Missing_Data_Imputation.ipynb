{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# KNN Imputation:\n",
        "\n",
        "* Overview: KNN (K-Nearest Neighbors) imputation fills in missing values by finding the 'k' most similar rows (neighbors) in the dataset and averaging their values.\n",
        "* Key Point: Simple and intuitive, but may struggle with high-dimensional data or datasets with outliers.\n",
        "\n",
        "**KNN Imputation General Guidelines**\n",
        "\n",
        "The authors of the KNN imputation algorithm suggest the following:\n",
        "\n",
        "* Small Amount of Missing Data: If less than 20% of your data is missing, KNN imputation will work more accurately.\n",
        "* Value of K: The specific number of neighbors (K) used in the imputation doesn’t need to be exact. As long as K is between 10 and 20, the results will generally be good.\n",
        "\n",
        "**Link for additional references**\n",
        "* [KNN](https://www.dropbox.com/scl/fo/le47x1fez8y7g7akw9bo9/AItqo18_QDZMGI39Lc8vdo0/Section-07-Multivariate-Imputation?dl=0&rlkey=7257ih8lct4v0nkroy7if74i1&subfolder_nav_tracking=1)"
      ],
      "metadata": {
        "id": "LnfVImLUopPX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KNN imputation with Scikitlearn\n",
        "\n",
        "**Additional notes**\n",
        "\n",
        "* Same K for All Variables: When using KNN imputation, the same number of neighbors (K) is applied to all variables with missing data. This means that the same setting of K is used uniformly across different features.\n",
        "\n",
        "* Optimizing K for Prediction: You can't easily optimize K specifically to improve the imputation of missing values in a single variable. However, you can optimize K to enhance the prediction of the target variable in the dataset.\n",
        "\n",
        "* Optimizing K for Target Prediction: While KNN imputation uses a fixed K for all variables, optimizing K might be more effective for predicting the target variable in a supervised learning context.\n",
        "\n",
        "\n",
        "**Why is it a regression problem?**\n",
        "\n",
        "In the context of imputation with separate KNN models, it becomes a regression problem because each variable with missing values is treated as a target variable that needs to be predicted. In simple termsn, when you impute missing values, you are essentially trying to predict what those missing values should be. This involves estimating a value based on the information available from other variables in the dataset.\n",
        "\n",
        "**Alternative Approach:**\n",
        "\n",
        "If the goal is to predict missing values as accurately as possible, it’s better to use separate KNN models for each variable with missing data. This approach involves treating each variable's imputation as a separate regression problem, where each variable is predicted based on other variables.\n",
        "\n",
        "* Single KNN Imputer: Uses one imputer for all missing data, treating all variables together.\n",
        "* Separate KNN Imputers: Uses distinct imputers for each variable, modeling each one individually based on the remaining variables."
      ],
      "metadata": {
        "id": "vmnTnNIBGGzu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Template"
      ],
      "metadata": {
        "id": "RPKW9gNfFZRQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import the KNNImputer from scikit-learn\n",
        "# This class is used for imputing missing values using the K-nearest neighbors algorithm\n",
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "# Step 2: Identify columns with missing data\n",
        "# This loop checks each column in the dataset for missing values and prints the column name and count of missing values if any\n",
        "for var in data.columns:\n",
        "    if data[var].isnull().sum() > 1:\n",
        "        print(var, data[var].isnull().sum())"
      ],
      "metadata": {
        "id": "m_H1rV26qYxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Separate the data into training and testing sets\n",
        "# Remove 'SalePrice' from the feature list as it is the target variable\n",
        "cols_to_use.remove('SalePrice')\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "# 30% of the data is used for testing, and the rest for training\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data[cols_to_use],  # Features for training\n",
        "    data['SalePrice'],  # Target variable\n",
        "    test_size=0.3,  # 30% test data\n",
        "    random_state=0)  # Seed for reproducibility\n",
        "\n",
        "# Step 4: Print the shape of the training and testing sets\n",
        "# Displays the number of rows and columns in the training and testing datasets\n",
        "X_train.shape, X_test.shape\n",
        "\n",
        "# Step 5: Reset index for training and testing sets\n",
        "# This ensures the indices are sequential and aligned, which is useful for comparison later\n",
        "X_train.reset_index(inplace=True, drop=True)\n",
        "X_test.reset_index(inplace=True, drop=True)"
      ],
      "metadata": {
        "id": "53ZN_BksF4Af"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Initialize the KNNImputer\n",
        "# This imputer replaces missing values using the K-nearest neighbors method\n",
        "# n_neighbors: Number of neighbors to use (K)\n",
        "# weights: How to weight the neighbors ('distance' means closer neighbors have more weight)\n",
        "# metric: Distance metric to use (nan_euclidean handles missing values)\n",
        "# add_indicator: Whether to add a column indicating missing values (False here)\n",
        "imputer = KNNImputer(\n",
        "    n_neighbors=5,  # Number of neighbors to consider\n",
        "    weights='distance',  # Weight neighbors by their distance\n",
        "    metric='nan_euclidean',  # Metric used to compute distance, handling NaNs\n",
        "    add_indicator=False  # Do not add a missing indicator column\n",
        ")\n",
        "\n",
        "# Step 2: Fit the imputer on the training data\n",
        "# The imputer learns the structure of the data to make imputations\n",
        "imputer.fit(X_train)\n",
        "\n",
        "# Step 3: Transform the training and testing data\n",
        "# Replaces missing values using the KNN algorithm\n",
        "train_t = imputer.transform(X_train)\n",
        "test_t = imputer.transform(X_test)\n",
        "\n",
        "# Step 4: Convert the results to DataFrames\n",
        "# The imputer returns a NumPy array, so we convert it to DataFrame for easier manipulation\n",
        "train_t = pd.DataFrame(train_t, columns=X_train.columns)\n",
        "test_t = pd.DataFrame(test_t, columns=X_test.columns)\n",
        "\n",
        "# Step 5: Display the first few rows of the transformed training data\n",
        "# Shows the imputed training data\n",
        "train_t.head()\n",
        "\n",
        "# Step 6: Check variables without NA after imputation\n",
        "# Counts the remaining missing values in specific columns of the transformed data\n",
        "train_t[['LotFrontage', 'MasVnrArea', 'GarageYrBlt']].isnull().sum()\n",
        "\n",
        "# Step 7: View original missing values in the training data\n",
        "# Displays the original missing values for 'MasVnrArea' in the training data\n",
        "X_train[X_train['MasVnrArea'].isnull()]['MasVnrArea']\n",
        "\n",
        "# Step 8: View imputed values for missing 'MasVnrArea'\n",
        "# Shows the imputed values for 'MasVnrArea' in the transformed training data\n",
        "train_t[X_train['MasVnrArea'].isnull()]['MasVnrArea']"
      ],
      "metadata": {
        "id": "DM9HaqUhGIUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KNN imputation - Feature engine\n",
        "\n",
        "* We can use Feature-engine to apply the KNNImputer to a slice of the dataframe."
      ],
      "metadata": {
        "id": "i3EGkkb5JR0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Initialize the SklearnTransformerWrapper with KNNImputer\n",
        "# This wrapper integrates the KNNImputer with Feature-engine, applying it to specified variables\n",
        "imputer = SklearnTransformerWrapper(\n",
        "    transformer=KNNImputer(weights='distance'),  # Use KNNImputer with distance-based weighting\n",
        "    variables=cols_to_use  # Apply imputation to these variables\n",
        ")\n",
        "\n",
        "# Step 2: Fit the wrapper and KNNImputer on the training data\n",
        "# This prepares the imputer by learning from the training data\n",
        "imputer.fit(X_train)\n",
        "\n",
        "# Step 3: Transform the training and testing data\n",
        "# Replaces missing values using the trained KNNImputer\n",
        "train_t = imputer.transform(X_train)\n",
        "test_t = imputer.transform(X_test)\n",
        "\n",
        "# Step 4: Display the first few rows of the transformed training data\n",
        "# Shows the imputed training data as a DataFrame\n",
        "train_t.head()\n",
        "\n",
        "# Step 5: Check for remaining missing values in 'MasVnrArea' after imputation\n",
        "# Verifies that there are no more missing values in the specified column\n",
        "train_t['MasVnrArea'].isnull().sum()"
      ],
      "metadata": {
        "id": "tu3BSZRPJfIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Automatically find best imputation parameters\n",
        "* We can optimise the parameters of the KNN imputation to better predict our outcome."
      ],
      "metadata": {
        "id": "ckuDhNaNK-dT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import extra classes for modelling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# separate intro train and test set\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data[cols_to_use],  # just the features\n",
        "    data['SalePrice'],  # the target\n",
        "    test_size=0.3,  # the percentage of obs in the test set\n",
        "    random_state=0)  # for reproducibility\n",
        "\n",
        "X_train.shape, X_test.shape"
      ],
      "metadata": {
        "id": "GPTQjhgMLCSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Define the pipeline\n",
        "# The pipeline includes an imputer, a scaler, and a regressor\n",
        "pipe = Pipeline(steps=[\n",
        "    ('imputer', KNNImputer(\n",
        "        n_neighbors=5,  # Number of neighbors for imputation\n",
        "        weights='distance',  # Weighting for neighbors\n",
        "        add_indicator=False  # Do not add missing indicator columns\n",
        "    )),\n",
        "\n",
        "    ('scaler', StandardScaler()),  # Standardize features\n",
        "    ('regressor', Lasso(max_iter=2000))  # Lasso regression with specified max iterations\n",
        "])\n",
        "\n",
        "# Step 2: Set up the parameter grid for GridSearchCV\n",
        "# Define the range of parameters to test for each step in the pipeline\n",
        "param_grid = {\n",
        "    'imputer__n_neighbors': [3, 5, 10],  # Different numbers of neighbors to test\n",
        "    'imputer__weights': ['uniform', 'distance'],  # Different weighting methods\n",
        "    'imputer__add_indicator': [True, False],  # Whether to add missing indicators\n",
        "    'regressor__alpha': [10, 100, 200],  # Regularization strength for Lasso regression\n",
        "}\n",
        "\n",
        "# Step 3: Initialize GridSearchCV\n",
        "# Perform an exhaustive search over the parameter grid with cross-validation\n",
        "grid_search = GridSearchCV(pipe, param_grid, cv=5, n_jobs=-1, scoring='r2')\n",
        "\n",
        "# Step 4: Fit GridSearchCV on the training data\n",
        "# Train the pipeline with all combinations of parameters to find the best model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Print the best score on the training set\n",
        "# Display the R^2 score of the best model on the training data\n",
        "print((\"Best linear regression from grid search: %.3f\"\n",
        "       % grid_search.score(X_train, y_train)))\n",
        "\n",
        "# Step 6: Print the performance on the test set\n",
        "# Display the R^2 score of the best model on the test data\n",
        "print((\"Best linear regression from grid search: %.3f\"\n",
        "       % grid_search.score(X_test, y_test)))\n",
        "\n",
        "# Step 7: Display the best parameters\n",
        "# Show the parameter combination that resulted in the best model\n",
        "grid_search.best_params_"
      ],
      "metadata": {
        "id": "6BClwWmsLLOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Key Differences**\n",
        "\n",
        "* First Code Snippet: Integrates KNN imputation within a pipeline that also includes scaling and regression. It performs grid search to optimize hyperparameters for both imputation and regression.\n",
        "* Second Code Snippet: Focuses solely on KNN imputation. It directly applies the imputer to the data without additional steps like scaling or regression."
      ],
      "metadata": {
        "id": "eeuvUbPQSxRC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MICE (Multiple Imputation by Chained Equations):\n",
        "\n",
        "**Additional notes**\n",
        "* Overview: MICE creates multiple datasets where missing values are imputed using models based on other variables. Each dataset reflects different plausible values for the missing data, capturing uncertainty.\n",
        "* Key Point: Generates multiple versions of the dataset and combines results for robust analysis.\n",
        "\n",
        "**MICE: Assumptions**\n",
        "* Data is MAR (Missing At Random): The missing data mechanism depends on observed data but not on the unobserved data.\n",
        "* Modeling Capability: The missing values in a variable can be predicted using other variables in the dataset, and do not rely on information from external sources.\n",
        "\n",
        "**Link for additional references**\n",
        "\n",
        "***To check variable nature, variable relationship, which variables should we use as predictors and more considerations, click the link***\n",
        "\n",
        "* [MICE](https://www.dropbox.com/scl/fo/le47x1fez8y7g7akw9bo9/AItqo18_QDZMGI39Lc8vdo0/Section-07-Multivariate-Imputation?dl=0&preview=02_MICE.pdf&rlkey=7257ih8lct4v0nkroy7if74i1&subfolder_nav_tracking=1)\n",
        "\n",
        "**Side notes**\n",
        "* We will implement MICE using various machine learning models to estimate the missing values.\n",
        "* Same model will be used to predict NA in all variables\n",
        "* Can't use classification for binary variables and regression for continuous variables\n",
        "* For a more sophisticated imputation, we would have to assemble the imputers / models manually."
      ],
      "metadata": {
        "id": "75UL1GZfopTF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# missForest:\n",
        "\n",
        "**Additional notes**\n",
        "* Overview: missForest is a machine learning-based method that uses random forests to predict and impute missing values. It iteratively improves predictions by refining the model with each iteration.\n",
        "* Key Point: Handles both categorical and numerical data effectively and can capture complex relationships between variables.\n",
        "\n",
        "**Link for additional references**\n",
        "* [missForest](https://www.dropbox.com/scl/fo/le47x1fez8y7g7akw9bo9/AItqo18_QDZMGI39Lc8vdo0/Section-07-Multivariate-Imputation?rlkey=7257ih8lct4v0nkroy7if74i1&subfolder_nav_tracking=1&dl=0)"
      ],
      "metadata": {
        "id": "ycdsuHz3opVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "\n",
        "# Step 1: Create an MICE imputer using Bayesian Ridge regression as the estimator\n",
        "# IterativeImputer performs multiple imputation by chained equations (MICE)\n",
        "imputer = IterativeImputer(\n",
        "    estimator=BayesianRidge(),  # Estimator used to predict missing values\n",
        "    initial_strategy='mean',    # Initial imputation strategy before iterative process\n",
        "    max_iter=10,                # Number of iterations for the imputation process\n",
        "    imputation_order='ascending', # Order in which variables are imputed\n",
        "    n_nearest_features=None,    # Number of features to consider for nearest neighbors\n",
        "    skip_complete=True,         # Skip variables that do not have missing values\n",
        "    random_state=0              # Seed for reproducibility\n",
        ")\n",
        "\n",
        "# Step 2: Fit the imputer on the training data\n",
        "# This process prepares the imputer by learning patterns to handle missing values\n",
        "imputer.fit(X_train)\n",
        "\n",
        "# Step 3: Transform the training and testing data\n",
        "# Replaces missing values in the datasets using the fitted imputer\n",
        "train_t = imputer.transform(X_train)\n",
        "test_t = imputer.transform(X_test)\n",
        "\n",
        "# Step 4: Verify that there are no missing values after imputation\n",
        "# Counts remaining missing values in the transformed training data\n",
        "pd.DataFrame(train_t, columns=X_train.columns).isnull().sum()"
      ],
      "metadata": {
        "id": "HZAg1uYYUbw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lets compare imputation with different models"
      ],
      "metadata": {
        "id": "H0AHtaoYbt0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "\n",
        "# Step 1: Split the data into training and testing sets\n",
        "# The target variable 'A16' is separated from the features\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data.drop('A16', axis=1),  # Features\n",
        "    data['A16'],                # Target\n",
        "    test_size=0.3,              # Proportion of data to be used for testing\n",
        "    random_state=0              # Seed for reproducibility\n",
        ")\n",
        "\n",
        "# Display the shape of the training and testing sets\n",
        "X_train.shape, X_test.shape\n",
        "\n",
        "# Step 2: Initialize multiple IterativeImputer instances with different estimators\n",
        "# These imputers will replace missing values using different regression models\n",
        "\n",
        "imputer_bayes = IterativeImputer(\n",
        "    estimator=BayesianRidge(),  # Bayesian Ridge regression as the estimator\n",
        "    max_iter=10,                # Number of iterations for the imputation process\n",
        "    random_state=0              # Seed for reproducibility\n",
        ")\n",
        "\n",
        "imputer_knn = IterativeImputer(\n",
        "    estimator=KNeighborsRegressor(n_neighbors=5),  # KNN regression as the estimator\n",
        "    max_iter=10,                                   # Number of iterations\n",
        "    random_state=0                                 # Seed for reproducibility\n",
        ")\n",
        "\n",
        "imputer_nonLin = IterativeImputer(\n",
        "    estimator=DecisionTreeRegressor(max_features='sqrt', random_state=0),  # Decision Tree regression as the estimator\n",
        "    max_iter=500,                                                          # Number of iterations\n",
        "    random_state=0                                                         # Seed for reproducibility\n",
        ")\n",
        "\n",
        "imputer_missForest = IterativeImputer(\n",
        "    estimator=ExtraTreesRegressor(n_estimators=10, random_state=0),  # Extra Trees regression as the estimator\n",
        "    max_iter=100,                                                    # Number of iterations\n",
        "    random_state=0                                                   # Seed for reproducibility\n",
        ")\n",
        "\n",
        "# Step 3: Fit each imputer on the training data\n",
        "# The imputers learn the data structure to replace missing values\n",
        "imputer_bayes.fit(X_train)\n",
        "imputer_knn.fit(X_train)\n",
        "imputer_nonLin.fit(X_train)\n",
        "imputer_missForest.fit(X_train)\n",
        "\n",
        "# Step 4: Transform the training data to replace missing values\n",
        "# Each imputer is used to fill missing values in the training set\n",
        "X_train_bayes = imputer_bayes.transform(X_train)\n",
        "X_train_knn = imputer_knn.transform(X_train)\n",
        "X_train_nonLin = imputer_nonLin.transform(X_train)\n",
        "X_train_missForest = imputer_missForest.transform(X_train)\n",
        "\n",
        "# Step 5: Convert transformed numpy arrays to DataFrames\n",
        "# Transform the numpy arrays into DataFrames with the same column names\n",
        "predictors = [var for var in X_train.columns if var != 'A16']  # List of predictor variables\n",
        "\n",
        "X_train_bayes = pd.DataFrame(X_train_bayes, columns=predictors)\n",
        "X_train_knn = pd.DataFrame(X_train_knn, columns=predictors)\n",
        "X_train_nonLin = pd.DataFrame(X_train_nonLin, columns=predictors)\n",
        "X_train_missForest = pd.DataFrame(X_train_missForest, columns=predictors)"
      ],
      "metadata": {
        "id": "dsAbteH_bvIO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}